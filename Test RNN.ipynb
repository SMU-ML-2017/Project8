{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify on Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      "Id                        568454 non-null int64\n",
      "ProductId                 568454 non-null object\n",
      "UserId                    568454 non-null object\n",
      "ProfileName               568438 non-null object\n",
      "HelpfulnessNumerator      568454 non-null int64\n",
      "HelpfulnessDenominator    568454 non-null int64\n",
      "Score                     568454 non-null int64\n",
      "Time                      568454 non-null int64\n",
      "Summary                   568427 non-null object\n",
      "Text                      568454 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "## Amazon Reviews\n",
    "## https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "reviews = pd.read_csv('data/Reviews.csv')\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del reviews['ProfileName']\n",
    "except KeyError:\n",
    "    print('No such column')\n",
    "    \n",
    "try:\n",
    "    del reviews['Summary']\n",
    "except KeyError:\n",
    "    print('No such column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44736</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64421</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "44736  44737  B001EQ55RW  A2V0I904FH7ABY                     3   \n",
       "64421  64422  B000MIDROQ  A161DK06JJMCYF                     3   \n",
       "\n",
       "       HelpfulnessDenominator  Score        Time  \\\n",
       "44736                       2      4  1212883200   \n",
       "64421                       1      5  1224892800   \n",
       "\n",
       "                                                    Text  \n",
       "44736  It was almost a 'love at first bite' - the per...  \n",
       "64421  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove rows where numerator > denominator. Assume this is entry error in dataset.\n",
    "faulty_rows = reviews[reviews['HelpfulnessNumerator'] > reviews['HelpfulnessDenominator']]\n",
    "reviews = reviews[reviews['HelpfulnessNumerator'] <= reviews['HelpfulnessDenominator']]\n",
    "\n",
    "faulty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# [CITE] https://www.evanmiller.org/how-not-to-sort-by-average-rating.html\n",
    "# \"The lower bound of Wilson score confidence interval for a Bernoulli parameter\"\n",
    "def lbc(positive_votes, total_votes):\n",
    "    negative_votes = total_votes - positive_votes\n",
    "    if total_votes == 0:\n",
    "        return 0.0\n",
    "    lower_bound = ((positive_votes + 1.9208) / (total_votes) - 1.96 * math.sqrt((total_votes * negative_votes) / (total_votes) + 0.9604) / \n",
    "        (total_votes)) / (1 + 3.8416 / (total_votes))\n",
    "    return lower_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.6 s, sys: 26.4 ms, total: 8.63 s\n",
      "Wall time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add the Lbc column\n",
    "lbc_for_row = lambda row: lbc(row['HelpfulnessNumerator'], row['HelpfulnessDenominator'])\n",
    "reviews['Lbc'] = reviews.apply(lbc_for_row, axis=1)\n",
    "\n",
    "# Make Score zero indexed\n",
    "reviews['Score'] = reviews['Score']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350460</th>\n",
       "      <td>350461</td>\n",
       "      <td>B003HV3VYQ</td>\n",
       "      <td>AYIIHXHBLN9EN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1312156800</td>\n",
       "      <td>Vanilla Maple is the absolute best flavor (it'...</td>\n",
       "      <td>0.438494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539520</th>\n",
       "      <td>539521</td>\n",
       "      <td>B006WYSFZK</td>\n",
       "      <td>A30H2335OM7RD6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1316649600</td>\n",
       "      <td>These are a favorite at our home. We like the ...</td>\n",
       "      <td>0.030218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347413</th>\n",
       "      <td>347414</td>\n",
       "      <td>B000BF3AGU</td>\n",
       "      <td>A1UPO54VSAC1Q4</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1215561600</td>\n",
       "      <td>I don't understand all the 5 star reviews here...</td>\n",
       "      <td>0.116686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82892</th>\n",
       "      <td>82893</td>\n",
       "      <td>B007TGDXNO</td>\n",
       "      <td>A2LCRIWBNR09X5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1345680000</td>\n",
       "      <td>This is a very good tasting coffee and I like ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530470</th>\n",
       "      <td>530471</td>\n",
       "      <td>B0017SZS8I</td>\n",
       "      <td>A2M6Q0M563E6V6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1307491200</td>\n",
       "      <td>'Mole' is a generic term for a very wide varie...</td>\n",
       "      <td>0.342372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "350460  350461  B003HV3VYQ   AYIIHXHBLN9EN                     3   \n",
       "539520  539521  B006WYSFZK  A30H2335OM7RD6                     1   \n",
       "347413  347414  B000BF3AGU  A1UPO54VSAC1Q4                     7   \n",
       "82892    82893  B007TGDXNO  A2LCRIWBNR09X5                     0   \n",
       "530470  530471  B0017SZS8I  A2M6Q0M563E6V6                     2   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "350460                       3      4  1312156800   \n",
       "539520                       2      4  1316649600   \n",
       "347413                      17      0  1215561600   \n",
       "82892                        0      4  1345680000   \n",
       "530470                       2      4  1307491200   \n",
       "\n",
       "                                                     Text       Lbc  \n",
       "350460  Vanilla Maple is the absolute best flavor (it'...  0.438494  \n",
       "539520  These are a favorite at our home. We like the ...  0.030218  \n",
       "347413  I don't understand all the 5 star reviews here...  0.116686  \n",
       "82892   This is a very good tasting coffee and I like ...  0.000000  \n",
       "530470  'Mole' is a generic term for a very wide varie...  0.342372  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reviews.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of quartile 715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I am on my second 5 Lb bag of beans from Palouse, and I will keep coming back as long as the price is consistent. The quality of the beans is high: they are delicious, cook well, are packed nicely, and arrived with Amazon Prime shipping-speed (two days). I don't take advantage of the QR code identifier (to see which specific fields the product comes from), but it's a nice touch, and builds my trust in Palouse, the manufacturer.<br /><br />Palouse is now my preferred option for both these garbanzo beans and brown lentils.<br /><br />Also: I have found one pebble going through 7.5 Lbs of beans--this is a very good ratio, and means they properly sift through their beans before packaging them. Worth the price.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartile = .85\n",
    "review_length = int(data['Text'].str.len().quantile(q=quartile))\n",
    "\n",
    "longest = data[data['Text'].str.len() == review_length]\n",
    "print(\"Length of quartile\", review_length)\n",
    "longest['Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133038 unique tokens. Distilled to 120000 top words.\n",
      "Shape of data tensor: (568452, 715)\n",
      "Shape of label tensor: (568452, 5)\n",
      "119999\n",
      "CPU times: user 43.9 s, sys: 840 ms, total: 44.8 s\n",
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = 120000\n",
    "MAX_ART_LEN = int(data['Text'].str.len().quantile(q=quartile)) # maximum and minimum number of words \n",
    "                                                               #  based on a quartile of review length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(data.Text)\n",
    "sequences = tokenizer.texts_to_sequences(data.Text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(data['Score'])\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454761, 715) (454761, 5)\n",
      "[  41814.   23815.   34112.   64523.  290497.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=data['Score'], \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = y_ohe.shape[1]\n",
    "print(X_train.shape,y_train_ohe.shape)\n",
    "print(np.sum(y_train_ohe,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(133039, 100)\n",
      "CPU times: user 6.96 s, sys: 128 ms, total: 7.09 s\n",
      "Wall time: 7.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# Define the embeding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               39900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 13,353,937\n",
      "Trainable params: 50,037\n",
      "Non-trainable params: 13,303,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import GRU                  # Recurrent Layers\n",
    "\n",
    "rnn1 = Sequential()\n",
    "rnn1.add(embedding_layer)\n",
    "rnn1.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn1.add(MaxPooling1D(pool_size=2))\n",
    "rnn1.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn1.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/4\n",
      "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
      "Epoch 2/4\n",
      "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
      "Epoch 3/4\n",
      "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
      "Epoch 4/4\n",
      "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
      "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
      "Wall time: 3h 26min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44ef714e48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rnn1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn1.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8280   309   496   103  1266]\n",
      " [ 2909   560  1150   323  1012]\n",
      " [ 1727   466  2599  1514  2222]\n",
      " [  695   101  1245  3969 10121]\n",
      " [ 1443    63   622  2097 68399]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 451s 10ms/step - loss: 1.0291 - acc: 0.6508 - val_loss: 0.8893 - val_acc: 0.6783\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 448s 10ms/step - loss: 0.8846 - acc: 0.6804 - val_loss: 0.8173 - val_acc: 0.6968\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 453s 10ms/step - loss: 0.8355 - acc: 0.6923 - val_loss: 0.7954 - val_acc: 0.7038\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 442s 10ms/step - loss: 0.8052 - acc: 0.6997 - val_loss: 0.7769 - val_acc: 0.7107\n",
    "\n",
    "CPU times: user 1h 44min 42s, sys: 27min 2s, total: 2h 11min 45s\n",
    "Wall time: 29min 56s\n",
    "```\n",
    "[[ 594    2  138   26  282]\n",
    " [ 187    5  183   58  165]\n",
    " [  87    4  265  182  316]\n",
    " [  32    1  152  315 1107]\n",
    " [  58    0  103  206 6901]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.9327 - acc: 0.6701 - val_loss: 0.8315 - val_acc: 0.6937\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.8090 - acc: 0.7014 - val_loss: 0.7651 - val_acc: 0.7136\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7749 - acc: 0.7120 - val_loss: 0.7545 - val_acc: 0.7154\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7517 - acc: 0.7191 - val_loss: 0.7304 - val_acc: 0.7266\n",
    "\n",
    "CPU times: user 1h 13min 8s, sys: 3min 12s, total: 1h 16min 21s\n",
    "Wall time: 52min 17s\n",
    "```\n",
    "[[ 2006    61    99    24   411]\n",
    " [  683   137   236    63   373]\n",
    " [  491   129   450   293   768]\n",
    " [  191    43   255   731  2820]\n",
    " [  350    23   141   318 17327]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 1547s 7ms/step - loss: 0.8767 - acc: 0.6832 - val_loss: 0.7632 - val_acc: 0.7135\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 1550s 7ms/step - loss: 0.7758 - acc: 0.7119 - val_loss: 0.7332 - val_acc: 0.7257\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 1554s 7ms/step - loss: 0.7447 - acc: 0.7217 - val_loss: 0.7117 - val_acc: 0.7352\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 1555s 7ms/step - loss: 0.7239 - acc: 0.7285 - val_loss: 0.7185 - val_acc: 0.7317\n",
    "\n",
    "CPU times: user 2h 30min, sys: 7min 19s, total: 2h 37min 20s\n",
    "Wall time: 1h 43min 26s\n",
    "```\n",
    "[[ 3375   348   204    40  1231]\n",
    " [  917   517   424   120  1007]\n",
    " [  426   332   933   578  1988]\n",
    " [  152    47   351  1209  6286]\n",
    " [  257    46   131   364 35563]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
    "\n",
    "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
    "Wall time: 3h 26min 27s\n",
    "```\n",
    "[[ 8280   309   496   103  1266]\n",
    " [ 2909   560  1150   323  1012]\n",
    " [ 1727   466  2599  1514  2222]\n",
    " [  695   101  1245  3969 10121]\n",
    " [ 1443    63   622  2097 68399]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 13,409,297\n",
      "Trainable params: 13,409,169\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "rnn2 = Sequential()\n",
    "rnn2.add(embedding_layer)\n",
    "rnn2.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(BatchNormalization())\n",
    "rnn2.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2, unroll=True))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/4\n",
      "454761/454761 [==============================] - 1045s 2ms/step - loss: 0.6070 - acc: 0.7728 - val_loss: 0.5951 - val_acc: 0.7805\n",
      "Epoch 2/4\n",
      " 73440/454761 [===>..........................] - ETA: 13:49 - loss: 0.5160 - acc: 0.8072"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rnn2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn2.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8332   882   598    50   592]\n",
      " [ 2046  1847  1444   209   408]\n",
      " [  791   827  4332  1484  1094]\n",
      " [  286   140  1557  6656  7492]\n",
      " [  762   130   839  3054 67839]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.9874 - acc: 0.6574 - val_loss: 0.9331 - val_acc: 0.6636\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.8625 - acc: 0.6846 - val_loss: 0.8611 - val_acc: 0.6892\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.8107 - acc: 0.6965 - val_loss: 0.8028 - val_acc: 0.6973\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.7780 - acc: 0.7080 - val_loss: 0.8506 - val_acc: 0.6994\n",
    "\n",
    "CPU times: user 56min 14s, sys: 14min 44s, total: 1h 10min 59s\n",
    "Wall time: 15min 49s\n",
    "```\n",
    "[[ 616   11   42   10  363]\n",
    " [ 198   10   64   39  287]\n",
    " [ 104    2   99   77  572]\n",
    " [  42    1   33   68 1463]\n",
    " [  58    0   25   27 7158]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 274s 2ms/step - loss: 0.9015 - acc: 0.6761 - val_loss: 0.8047 - val_acc: 0.6987\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7932 - acc: 0.7021 - val_loss: 0.7815 - val_acc: 0.7025\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7555 - acc: 0.7145 - val_loss: 0.8046 - val_acc: 0.6960\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7293 - acc: 0.7245 - val_loss: 0.7831 - val_acc: 0.7154\n",
    "\n",
    "CPU times: user 26min 3s, sys: 1min 13s, total: 27min 16s\n",
    "Wall time: 18min 13s\n",
    "```\n",
    "[[ 1551    62    85    23   880]\n",
    " [  509   110   230    70   573]\n",
    " [  237    91   373   326  1104]\n",
    " [   78    28   177   599  3158]\n",
    " [  128    13    83   233 17702]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 543s 2ms/step - loss: 0.8495 - acc: 0.6899 - val_loss: 0.7786 - val_acc: 0.7036\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7544 - acc: 0.7170 - val_loss: 0.7779 - val_acc: 0.6993\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7212 - acc: 0.7283 - val_loss: 0.7270 - val_acc: 0.7259\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 544s 2ms/step - loss: 0.6999 - acc: 0.7352 - val_loss: 0.7443 - val_acc: 0.7299\n",
    "\n",
    "CPU times: user 50min 21s, sys: 2min 18s, total: 52min 40s\n",
    "Wall time: 36min 18s\n",
    "```\n",
    "[[ 3708    70   204    67  1149]\n",
    " [ 1245   127   454   194   965]\n",
    " [  618   104   753   865  1917]\n",
    " [  173    18   270  1570  6014]\n",
    " [  313     4   102   606 35336]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1085s 2ms/step - loss: 0.8098 - acc: 0.7001 - val_loss: 0.7436 - val_acc: 0.7193\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.7243 - acc: 0.7271 - val_loss: 0.7504 - val_acc: 0.7266\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6966 - acc: 0.7364 - val_loss: 0.7103 - val_acc: 0.7338\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6782 - acc: 0.7429 - val_loss: 0.6988 - val_acc: 0.7354\n",
    "\n",
    "CPU times: user 1h 44min 8s, sys: 4min 53s, total: 1h 49min 1s\n",
    "Wall time: 1h 12min 31s\n",
    "```\n",
    "[[ 6899   726  1663    93  1073]\n",
    " [ 1535   781  2684   249   705]\n",
    " [  728   329  4397  1602  1472]\n",
    " [  266    47  2311  4983  8524]\n",
    " [  792    44  1657  3577 66554]]\n",
    " ```\n",
    " ## Training Embedding\n",
    " #### 2 Epochs\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/2\n",
    "454761/454761 [==============================] - 1038s 2ms/step - loss: 0.7174 - acc: 0.7310 - val_loss: 0.6536 - val_acc: 0.7576\n",
    "\n",
    "Epoch 2/2\n",
    "454761/454761 [==============================] - 1035s 2ms/step - loss: 0.5844 - acc: 0.7799 - val_loss: 0.5877 - val_acc: 0.7829\n",
    "\n",
    "CPU times: user 35min 26s, sys: 1min 36s, total: 37min 2s\n",
    "Wall time: 34min 43s\n",
    "```\n",
    "[[ 8332   882   598    50   592]\n",
    " [ 2046  1847  1444   209   408]\n",
    " [  791   827  4332  1484  1094]\n",
    " [  286   140  1557  6656  7492]\n",
    " [  762   130   839  3054 67839]]\n",
    " ```\n",
    " #### 4 Epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
