{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify on Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      "Id                        568454 non-null int64\n",
      "ProductId                 568454 non-null object\n",
      "UserId                    568454 non-null object\n",
      "ProfileName               568438 non-null object\n",
      "HelpfulnessNumerator      568454 non-null int64\n",
      "HelpfulnessDenominator    568454 non-null int64\n",
      "Score                     568454 non-null int64\n",
      "Time                      568454 non-null int64\n",
      "Summary                   568427 non-null object\n",
      "Text                      568454 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "## Amazon Reviews\n",
    "## https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "reviews = pd.read_csv('data/Reviews.csv')\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del reviews['ProfileName']\n",
    "except KeyError:\n",
    "    print('No such column')\n",
    "    \n",
    "try:\n",
    "    del reviews['Summary']\n",
    "except KeyError:\n",
    "    print('No such column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44736</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64421</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "44736  44737  B001EQ55RW  A2V0I904FH7ABY                     3   \n",
       "64421  64422  B000MIDROQ  A161DK06JJMCYF                     3   \n",
       "\n",
       "       HelpfulnessDenominator  Score        Time  \\\n",
       "44736                       2      4  1212883200   \n",
       "64421                       1      5  1224892800   \n",
       "\n",
       "                                                    Text  \n",
       "44736  It was almost a 'love at first bite' - the per...  \n",
       "64421  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove rows where numerator > denominator. Assume this is entry error in dataset.\n",
    "faulty_rows = reviews[reviews['HelpfulnessNumerator'] > reviews['HelpfulnessDenominator']]\n",
    "reviews = reviews[reviews['HelpfulnessNumerator'] <= reviews['HelpfulnessDenominator']]\n",
    "\n",
    "faulty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# [CITE] https://www.evanmiller.org/how-not-to-sort-by-average-rating.html\n",
    "# \"The lower bound of Wilson score confidence interval for a Bernoulli parameter\"\n",
    "def lbc(positive_votes, total_votes):\n",
    "    negative_votes = total_votes - positive_votes\n",
    "    if total_votes == 0:\n",
    "        return 0.0\n",
    "    lower_bound = ((positive_votes + 1.9208) / (total_votes) - 1.96 * math.sqrt((total_votes * negative_votes) / (total_votes) + 0.9604) / \n",
    "        (total_votes)) / (1 + 3.8416 / (total_votes))\n",
    "    return lower_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.83 s, sys: 11 ms, total: 8.84 s\n",
      "Wall time: 8.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add the Lbc column\n",
    "lbc_for_row = lambda row: lbc(row['HelpfulnessNumerator'], row['HelpfulnessDenominator'])\n",
    "reviews['Lbc'] = reviews.apply(lbc_for_row, axis=1)\n",
    "\n",
    "# Make Score zero indexed\n",
    "reviews['Score'] = reviews['Score']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43047</th>\n",
       "      <td>43048</td>\n",
       "      <td>B004P0SRSS</td>\n",
       "      <td>A264CH3T20YH3Z</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1316131200</td>\n",
       "      <td>Also found these while touring Europe.  Love t...</td>\n",
       "      <td>0.438494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279394</th>\n",
       "      <td>279395</td>\n",
       "      <td>B0039556DY</td>\n",
       "      <td>A255LN7YXS7QAE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1295481600</td>\n",
       "      <td>I drink only flavored caffeinated coffee.  I s...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412922</th>\n",
       "      <td>412923</td>\n",
       "      <td>B000E4C2LW</td>\n",
       "      <td>A3TXDO9392M8NJ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1275350400</td>\n",
       "      <td>I know what you're thinking.  Corn flakes are ...</td>\n",
       "      <td>0.206543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326560</th>\n",
       "      <td>326561</td>\n",
       "      <td>B002AQ0OW6</td>\n",
       "      <td>A3RCJ8SLVUKT7U</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1290729600</td>\n",
       "      <td>Have ordered boxes on 3 occassions. Every box ...</td>\n",
       "      <td>0.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329343</th>\n",
       "      <td>329344</td>\n",
       "      <td>B000V762EA</td>\n",
       "      <td>A255ZAR2NXQY7W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1346803200</td>\n",
       "      <td>I was at Gilt (in Portland, OR) enjoying some ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "43047    43048  B004P0SRSS  A264CH3T20YH3Z                     3   \n",
       "279394  279395  B0039556DY  A255LN7YXS7QAE                     0   \n",
       "412922  412923  B000E4C2LW  A3TXDO9392M8NJ                     1   \n",
       "326560  326561  B002AQ0OW6  A3RCJ8SLVUKT7U                     4   \n",
       "329343  329344  B000V762EA  A255ZAR2NXQY7W                     0   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "43047                        3      4  1316131200   \n",
       "279394                       0      4  1295481600   \n",
       "412922                       1      4  1275350400   \n",
       "326560                       4      0  1290729600   \n",
       "329343                       0      4  1346803200   \n",
       "\n",
       "                                                     Text       Lbc  \n",
       "43047   Also found these while touring Europe.  Love t...  0.438494  \n",
       "279394  I drink only flavored caffeinated coffee.  I s...  0.000000  \n",
       "412922  I know what you're thinking.  Corn flakes are ...  0.206543  \n",
       "326560  Have ordered boxes on 3 occassions. Every box ...  0.510100  \n",
       "329343  I was at Gilt (in Portland, OR) enjoying some ...  0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reviews.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of quartile 715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"These are a great alternative to Maruchan, which are fried ramen noodles.  If you are like me and  love ramen noodles, but hate the fact that there's a ton of fat (which is 14 grams per pack and 1660 mg of sodium for chicken flavor), try these.  I drain the water from my noodles and season them.  I do not eat it as soup.  I use half of the seasoning pack and a couple of spritzes of liquid aminos (tastes like soy sauce).  It ends up having 1 gram of fat and about 50% less sodium, and is very tasty.  Now those like me who like to cut some fat and salt can have their ramen noodles and eat them, too!  ; D.<br /> Ohh Haapppyyy Daayyy<br /><br />Now if only someone will make a Snickers with one gram of fat.  : (\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartile = .85\n",
    "review_length = int(data['Text'].str.len().quantile(q=quartile))\n",
    "\n",
    "longest = data[data['Text'].str.len() == review_length]\n",
    "print(\"Length of quartile\", review_length)\n",
    "longest['Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133038 unique tokens. Distilled to 120000 top words.\n",
      "Shape of data tensor: (568452, 715)\n",
      "119999\n",
      "CPU times: user 43 s, sys: 799 ms, total: 43.8 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = 120000\n",
    "MAX_ART_LEN = int(data['Text'].str.len().quantile(q=quartile)) # maximum and minimum number of words \n",
    "                                                               #  based on a quartile of review length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(data.Text)\n",
    "sequences = tokenizer.texts_to_sequences(data.Text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454761, 715) (454761,)\n",
      "289519.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "\n",
    "#######################\n",
    "## Ordinal Selection ##\n",
    "ordinal = True\n",
    "#######################\n",
    "\n",
    "if ordinal:\n",
    "    y = data['Score']/5\n",
    "else:\n",
    "    y = keras.utils.to_categorical(data['Score'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                            stratify=data['Score'], \n",
    "                                                            random_state=42)\n",
    "if ordinal:\n",
    "    loss='mean_squared_error'\n",
    "    NUM_CLASSES = 1\n",
    "else:\n",
    "    loss='categorical_crossentropy'\n",
    "    NUM_CLASSES = y.shape[1]\n",
    "    \n",
    "print(X_train.shape,y_train.shape)\n",
    "print(np.sum(y_train,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(133039, 100)\n",
      "CPU times: user 7.43 s, sys: 104 ms, total: 7.53 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# Define the embeding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "gru_embedding = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)\n",
    "\n",
    "lstm_embedding = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cache/Fit Function\n",
    "Load the cached mode or fit if no model found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from keras.models import load_model\n",
    "import json\n",
    "\n",
    "def cache_fit(model_name: str, model, *args, **kwargs):\n",
    "    archive_name = f'{model_name}_model.h5'\n",
    "    history_name = f'{model_name}_history.json'\n",
    "    archive_exists = os.path.isfile(archive_name)\n",
    "\n",
    "    if not archive_exists:\n",
    "        print(f'Model {model_name} not found in archive. Training new model.')\n",
    "        hist = model.fit(*args, **kwargs)\n",
    "        model.save(archive_name)\n",
    "        with open(history_name, 'w') as f:\n",
    "            json.dump(hist.history, f)\n",
    "        return model\n",
    "    else:\n",
    "        print('Model found on disk. Reloading.')\n",
    "        return load_model(archive_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 100)               49500     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 13,392,537\n",
      "Trainable params: 13,392,409\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model gru not found in archive. Training new model.\n",
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/2\n",
      "454761/454761 [==============================] - 1077s 2ms/step - loss: 0.0259 - acc: 0.0803 - val_loss: 0.0231 - val_acc: 0.0776\n",
      "Epoch 2/2\n",
      "454761/454761 [==============================] - 1076s 2ms/step - loss: 0.0172 - acc: 0.0868 - val_loss: 0.0226 - val_acc: 0.0767\n",
      "CPU times: user 48min 37s, sys: 3min 3s, total: 51min 40s\n",
      "Wall time: 35min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import GRU                  # Recurrent Layers\n",
    "\n",
    "gru = Sequential()\n",
    "gru.add(gru_embedding)\n",
    "gru.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "gru.add(MaxPooling1D(pool_size=2))\n",
    "gru.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "gru.add(MaxPooling1D(pool_size=2))\n",
    "gru.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "gru.add(MaxPooling1D(pool_size=2))\n",
    "gru.add(BatchNormalization())\n",
    "gru.add(GRU(100,dropout=0.25, recurrent_dropout=0.2))\n",
    "gru.add(Dense(64))\n",
    "gru.add(Dense(64))\n",
    "gru.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "gru.compile(loss=loss,\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "gru.summary()\n",
    "\n",
    "gru = cache_fit(\n",
    "    'gru', gru, \n",
    "    X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 451s 10ms/step - loss: 1.0291 - acc: 0.6508 - val_loss: 0.8893 - val_acc: 0.6783\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 448s 10ms/step - loss: 0.8846 - acc: 0.6804 - val_loss: 0.8173 - val_acc: 0.6968\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 453s 10ms/step - loss: 0.8355 - acc: 0.6923 - val_loss: 0.7954 - val_acc: 0.7038\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 442s 10ms/step - loss: 0.8052 - acc: 0.6997 - val_loss: 0.7769 - val_acc: 0.7107\n",
    "\n",
    "CPU times: user 1h 44min 42s, sys: 27min 2s, total: 2h 11min 45s\n",
    "Wall time: 29min 56s\n",
    "```\n",
    "[[ 594    2  138   26  282]\n",
    " [ 187    5  183   58  165]\n",
    " [  87    4  265  182  316]\n",
    " [  32    1  152  315 1107]\n",
    " [  58    0  103  206 6901]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.9327 - acc: 0.6701 - val_loss: 0.8315 - val_acc: 0.6937\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.8090 - acc: 0.7014 - val_loss: 0.7651 - val_acc: 0.7136\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7749 - acc: 0.7120 - val_loss: 0.7545 - val_acc: 0.7154\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7517 - acc: 0.7191 - val_loss: 0.7304 - val_acc: 0.7266\n",
    "\n",
    "CPU times: user 1h 13min 8s, sys: 3min 12s, total: 1h 16min 21s\n",
    "Wall time: 52min 17s\n",
    "```\n",
    "[[ 2006    61    99    24   411]\n",
    " [  683   137   236    63   373]\n",
    " [  491   129   450   293   768]\n",
    " [  191    43   255   731  2820]\n",
    " [  350    23   141   318 17327]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 1547s 7ms/step - loss: 0.8767 - acc: 0.6832 - val_loss: 0.7632 - val_acc: 0.7135\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 1550s 7ms/step - loss: 0.7758 - acc: 0.7119 - val_loss: 0.7332 - val_acc: 0.7257\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 1554s 7ms/step - loss: 0.7447 - acc: 0.7217 - val_loss: 0.7117 - val_acc: 0.7352\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 1555s 7ms/step - loss: 0.7239 - acc: 0.7285 - val_loss: 0.7185 - val_acc: 0.7317\n",
    "\n",
    "CPU times: user 2h 30min, sys: 7min 19s, total: 2h 37min 20s\n",
    "Wall time: 1h 43min 26s\n",
    "```\n",
    "[[ 3375   348   204    40  1231]\n",
    " [  917   517   424   120  1007]\n",
    " [  426   332   933   578  1988]\n",
    " [  152    47   351  1209  6286]\n",
    " [  257    46   131   364 35563]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
    "\n",
    "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
    "Wall time: 3h 26min 27s\n",
    "```\n",
    "[[ 8280   309   496   103  1266]\n",
    " [ 2909   560  1150   323  1012]\n",
    " [ 1727   466  2599  1514  2222]\n",
    " [  695   101  1245  3969 10121]\n",
    " [ 1443    63   622  2097 68399]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 13,409,037\n",
      "Trainable params: 13,408,909\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model lstm not found in archive. Training new model.\n",
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/2\n",
      "454761/454761 [==============================] - 1287s 3ms/step - loss: 0.0252 - acc: 0.0806 - val_loss: 0.0185 - val_acc: 0.0847\n",
      "Epoch 2/2\n",
      "454761/454761 [==============================] - 1287s 3ms/step - loss: 0.0165 - acc: 0.0869 - val_loss: 0.0169 - val_acc: 0.0854\n",
      "CPU times: user 57min 33s, sys: 3min 19s, total: 1h 53s\n",
      "Wall time: 42min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(lstm_embedding)\n",
    "lstm.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "lstm.add(MaxPooling1D(pool_size=2))\n",
    "lstm.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "lstm.add(MaxPooling1D(pool_size=2))\n",
    "lstm.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "lstm.add(MaxPooling1D(pool_size=2))\n",
    "lstm.add(BatchNormalization())\n",
    "lstm.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2))\n",
    "lstm.add(Dense(64))\n",
    "lstm.add(Dense(64))\n",
    "lstm.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "lstm.compile(loss=loss,\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "lstm.summary()\n",
    "\n",
    "lstm = cache_fit(\n",
    "    'lstm', lstm, \n",
    "    X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.9874 - acc: 0.6574 - val_loss: 0.9331 - val_acc: 0.6636\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.8625 - acc: 0.6846 - val_loss: 0.8611 - val_acc: 0.6892\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.8107 - acc: 0.6965 - val_loss: 0.8028 - val_acc: 0.6973\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.7780 - acc: 0.7080 - val_loss: 0.8506 - val_acc: 0.6994\n",
    "\n",
    "CPU times: user 56min 14s, sys: 14min 44s, total: 1h 10min 59s\n",
    "Wall time: 15min 49s\n",
    "```\n",
    "[[ 616   11   42   10  363]\n",
    " [ 198   10   64   39  287]\n",
    " [ 104    2   99   77  572]\n",
    " [  42    1   33   68 1463]\n",
    " [  58    0   25   27 7158]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 274s 2ms/step - loss: 0.9015 - acc: 0.6761 - val_loss: 0.8047 - val_acc: 0.6987\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7932 - acc: 0.7021 - val_loss: 0.7815 - val_acc: 0.7025\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7555 - acc: 0.7145 - val_loss: 0.8046 - val_acc: 0.6960\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7293 - acc: 0.7245 - val_loss: 0.7831 - val_acc: 0.7154\n",
    "\n",
    "CPU times: user 26min 3s, sys: 1min 13s, total: 27min 16s\n",
    "Wall time: 18min 13s\n",
    "```\n",
    "[[ 1551    62    85    23   880]\n",
    " [  509   110   230    70   573]\n",
    " [  237    91   373   326  1104]\n",
    " [   78    28   177   599  3158]\n",
    " [  128    13    83   233 17702]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 543s 2ms/step - loss: 0.8495 - acc: 0.6899 - val_loss: 0.7786 - val_acc: 0.7036\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7544 - acc: 0.7170 - val_loss: 0.7779 - val_acc: 0.6993\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7212 - acc: 0.7283 - val_loss: 0.7270 - val_acc: 0.7259\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 544s 2ms/step - loss: 0.6999 - acc: 0.7352 - val_loss: 0.7443 - val_acc: 0.7299\n",
    "\n",
    "CPU times: user 50min 21s, sys: 2min 18s, total: 52min 40s\n",
    "Wall time: 36min 18s\n",
    "```\n",
    "[[ 3708    70   204    67  1149]\n",
    " [ 1245   127   454   194   965]\n",
    " [  618   104   753   865  1917]\n",
    " [  173    18   270  1570  6014]\n",
    " [  313     4   102   606 35336]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1085s 2ms/step - loss: 0.8098 - acc: 0.7001 - val_loss: 0.7436 - val_acc: 0.7193\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.7243 - acc: 0.7271 - val_loss: 0.7504 - val_acc: 0.7266\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6966 - acc: 0.7364 - val_loss: 0.7103 - val_acc: 0.7338\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6782 - acc: 0.7429 - val_loss: 0.6988 - val_acc: 0.7354\n",
    "\n",
    "CPU times: user 1h 44min 8s, sys: 4min 53s, total: 1h 49min 1s\n",
    "Wall time: 1h 12min 31s\n",
    "```\n",
    "[[ 6899   726  1663    93  1073]\n",
    " [ 1535   781  2684   249   705]\n",
    " [  728   329  4397  1602  1472]\n",
    " [  266    47  2311  4983  8524]\n",
    " [  792    44  1657  3577 66554]]\n",
    " ```\n",
    " ## Training Embedding\n",
    " #### 2 Epochs\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/2\n",
    "454761/454761 [==============================] - 1038s 2ms/step - loss: 0.7174 - acc: 0.7310 - val_loss: 0.6536 - val_acc: 0.7576\n",
    "\n",
    "Epoch 2/2\n",
    "454761/454761 [==============================] - 1035s 2ms/step - loss: 0.5844 - acc: 0.7799 - val_loss: 0.5877 - val_acc: 0.7829\n",
    "\n",
    "CPU times: user 35min 26s, sys: 1min 36s, total: 37min 2s\n",
    "Wall time: 34min 43s\n",
    "```\n",
    "[[ 8332   882   598    50   592]\n",
    " [ 2046  1847  1444   209   408]\n",
    " [  791   827  4332  1484  1094]\n",
    " [  286   140  1557  6656  7492]\n",
    " [  762   130   839  3054 67839]]\n",
    " ```\n",
    " #### 4 Epochs\n",
    "Train on 454761 samples, validate on 113691 samples\n",
    "\n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1039s 2ms/step - loss: 0.7166 - acc: 0.7314 - val_loss: 0.6305 - val_acc: 0.7638\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.5836 - acc: 0.7805 - val_loss: 0.5858 - val_acc: 0.7810\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.5079 - acc: 0.8119 - val_loss: 0.5684 - val_acc: 0.7955\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.4453 - acc: 0.8384 - val_loss: 0.5913 - val_acc: 0.7899\n",
    "\n",
    "CPU times: user 1h 10min 36s, sys: 3min 19s, total: 1h 13min 56s\n",
    "Wall time: 1h 9min 17s\n",
    "```\n",
    "[[ 7640  1949   308    59   498]\n",
    " [ 1124  3690   665   112   363]\n",
    " [  604  1982  4115   965   862]\n",
    " [  274   498  1659  7581  6119]\n",
    " [  734   537  1011  3568 66774]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from keras.models import load_model\n",
    "import json\n",
    "\n",
    "def cache_fit(model_name: str, model, *args, **kwargs):\n",
    "    archive_name = f'{model_name}_model.h5'\n",
    "    history_name = f'{model_name}_history.json'\n",
    "    archive_exists = os.path.isfile(archive_name)\n",
    "\n",
    "    if not archive_exists:\n",
    "        print(f'Model {model_name} not found in archive. Training new model.')\n",
    "        hist = model.fit(*args, **kwargs)\n",
    "        model.save(archive_name)\n",
    "        with open(history_name, 'w') as f:\n",
    "            json.dump(hist.history, f)\n",
    "        return model\n",
    "    else:\n",
    "        print('Model found on disk. Reloading.')\n",
    "        return load_model(archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found on disk. Reloading.\n"
     ]
    }
   ],
   "source": [
    "model = cache_fit(\n",
    "    'rnn3', rnn3, \n",
    "    X_train, y_train_ordinal, validation_data=(X_test, y_test_ordinal), epochs=2, batch_size=32\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
