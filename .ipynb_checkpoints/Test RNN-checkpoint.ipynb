{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify on Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      "Id                        568454 non-null int64\n",
      "ProductId                 568454 non-null object\n",
      "UserId                    568454 non-null object\n",
      "ProfileName               568438 non-null object\n",
      "HelpfulnessNumerator      568454 non-null int64\n",
      "HelpfulnessDenominator    568454 non-null int64\n",
      "Score                     568454 non-null int64\n",
      "Time                      568454 non-null int64\n",
      "Summary                   568427 non-null object\n",
      "Text                      568454 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "## Amazon Reviews\n",
    "## https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "reviews = pd.read_csv('data/Reviews.csv')\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del reviews['ProfileName']\n",
    "except KeyError:\n",
    "    print('No such column')\n",
    "    \n",
    "try:\n",
    "    del reviews['Summary']\n",
    "except KeyError:\n",
    "    print('No such column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44736</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64421</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "44736  44737  B001EQ55RW  A2V0I904FH7ABY                     3   \n",
       "64421  64422  B000MIDROQ  A161DK06JJMCYF                     3   \n",
       "\n",
       "       HelpfulnessDenominator  Score        Time  \\\n",
       "44736                       2      4  1212883200   \n",
       "64421                       1      5  1224892800   \n",
       "\n",
       "                                                    Text  \n",
       "44736  It was almost a 'love at first bite' - the per...  \n",
       "64421  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove rows where numerator > denominator. Assume this is entry error in dataset.\n",
    "faulty_rows = reviews[reviews['HelpfulnessNumerator'] > reviews['HelpfulnessDenominator']]\n",
    "reviews = reviews[reviews['HelpfulnessNumerator'] <= reviews['HelpfulnessDenominator']]\n",
    "\n",
    "faulty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# [CITE] https://www.evanmiller.org/how-not-to-sort-by-average-rating.html\n",
    "# \"The lower bound of Wilson score confidence interval for a Bernoulli parameter\"\n",
    "def lbc(positive_votes, total_votes):\n",
    "    negative_votes = total_votes - positive_votes\n",
    "    if total_votes == 0:\n",
    "        return 0.0\n",
    "    lower_bound = ((positive_votes + 1.9208) / (total_votes) - 1.96 * math.sqrt((total_votes * negative_votes) / (total_votes) + 0.9604) / \n",
    "        (total_votes)) / (1 + 3.8416 / (total_votes))\n",
    "    return lower_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.38 s, sys: 43.4 ms, total: 8.43 s\n",
      "Wall time: 8.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add the Lbc column\n",
    "lbc_for_row = lambda row: lbc(row['HelpfulnessNumerator'], row['HelpfulnessDenominator'])\n",
    "reviews['Lbc'] = reviews.apply(lbc_for_row, axis=1)\n",
    "\n",
    "# Make Score zero indexed\n",
    "reviews['Score'] = reviews['Score']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395895</th>\n",
       "      <td>395896</td>\n",
       "      <td>B001CWSKFC</td>\n",
       "      <td>A197OKILYY934X</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1298592000</td>\n",
       "      <td>I recently discovered these at a local Walmart...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9837</th>\n",
       "      <td>9838</td>\n",
       "      <td>B003W07D1E</td>\n",
       "      <td>A27CKOACMX8O5F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1301356800</td>\n",
       "      <td>Love the cinnamon taste but way too sweet. I g...</td>\n",
       "      <td>-0.170084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217617</th>\n",
       "      <td>217618</td>\n",
       "      <td>B000LQLV7E</td>\n",
       "      <td>A5O3M8WH3WLMW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1346371200</td>\n",
       "      <td>good shipment system&lt;br /&gt;nice taste&lt;br /&gt;hot ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39973</th>\n",
       "      <td>39974</td>\n",
       "      <td>B001TZJ3OE</td>\n",
       "      <td>A20824UL50NSJB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1167782400</td>\n",
       "      <td>kids love the sauce on many things... great on...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328259</th>\n",
       "      <td>328260</td>\n",
       "      <td>B0001AO9LA</td>\n",
       "      <td>A3EIE7D4PET3B6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1346889600</td>\n",
       "      <td>I don't know what other reviewers received, bu...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "395895  395896  B001CWSKFC  A197OKILYY934X                     0   \n",
       "9837      9838  B003W07D1E  A27CKOACMX8O5F                     0   \n",
       "217617  217618  B000LQLV7E   A5O3M8WH3WLMW                     0   \n",
       "39973    39974  B001TZJ3OE  A20824UL50NSJB                     0   \n",
       "328259  328260  B0001AO9LA  A3EIE7D4PET3B6                     0   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "395895                       0      4  1298592000   \n",
       "9837                         1      1  1301356800   \n",
       "217617                       0      4  1346371200   \n",
       "39973                        0      4  1167782400   \n",
       "328259                       0      4  1346889600   \n",
       "\n",
       "                                                     Text       Lbc  \n",
       "395895  I recently discovered these at a local Walmart...  0.000000  \n",
       "9837    Love the cinnamon taste but way too sweet. I g... -0.170084  \n",
       "217617  good shipment system<br />nice taste<br />hot ...  0.000000  \n",
       "39973   kids love the sauce on many things... great on...  0.000000  \n",
       "328259  I don't know what other reviewers received, bu...  0.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reviews.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of quartile 715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I generally like vanilla flavoured beverages but I always take coffee black and unsweetened, I thought this would be a nice change and a combination of the 2.  Not so.  It just tasted .. well, wrong.  It tasted like weak custard which I guess is evidence that the french vanilla flavour is there but, for me, custard should remain a topping for apple pie, not in my coffee... LOL!  I do like Folger's Cappuccino flavoured beverages but as they are loaded with sugar I don't drink them that often and if I do I like them with cold with blended ice.  I have the hazelnut in this too which I find more palatable but I guess I will stick to straight black coffee from now on... my favourite being Illy Espresso.. yummy!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartile = .85\n",
    "review_length = int(data['Text'].str.len().quantile(q=quartile))\n",
    "\n",
    "longest = data[data['Text'].str.len() == review_length]\n",
    "print(\"Length of quartile\", review_length)\n",
    "longest['Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133038 unique tokens. Distilled to 120000 top words.\n",
      "Shape of data tensor: (568452, 715)\n",
      "Shape of label tensor: (568452, 5)\n",
      "119999\n",
      "CPU times: user 42.9 s, sys: 780 ms, total: 43.7 s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = 120000\n",
    "MAX_ART_LEN = int(data['Text'].str.len().quantile(q=quartile)) # maximum and minimum number of words \n",
    "                                                               #  based on a quartile of review length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(data.Text)\n",
    "sequences = tokenizer.texts_to_sequences(data.Text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(data['Score'])\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454761, 715) (454761, 5)\n",
      "[  41814.   23815.   34112.   64523.  290497.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=data['Score'], \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = y_ohe.shape[1]\n",
    "print(X_train.shape,y_train_ohe.shape)\n",
    "print(np.sum(y_train_ohe,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(133039, 100)\n",
      "CPU times: user 7.18 s, sys: 82.2 ms, total: 7.27 s\n",
      "Wall time: 7.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# Define the embeding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer1 = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)\n",
    "\n",
    "embedding_layer2 = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)\n",
    "\n",
    "embedding_layer3 = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               39900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 13,353,937\n",
      "Trainable params: 50,037\n",
      "Non-trainable params: 13,303,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import GRU                  # Recurrent Layers\n",
    "\n",
    "rnn1 = Sequential()\n",
    "rnn1.add(embedding_layer1)\n",
    "rnn1.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn1.add(MaxPooling1D(pool_size=2))\n",
    "rnn1.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn1.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/4\n",
      "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
      "Epoch 2/4\n",
      "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
      "Epoch 3/4\n",
      "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
      "Epoch 4/4\n",
      "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
      "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
      "Wall time: 3h 26min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44ef714e48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history1 = rnn1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn1.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8280   309   496   103  1266]\n",
      " [ 2909   560  1150   323  1012]\n",
      " [ 1727   466  2599  1514  2222]\n",
      " [  695   101  1245  3969 10121]\n",
      " [ 1443    63   622  2097 68399]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 451s 10ms/step - loss: 1.0291 - acc: 0.6508 - val_loss: 0.8893 - val_acc: 0.6783\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 448s 10ms/step - loss: 0.8846 - acc: 0.6804 - val_loss: 0.8173 - val_acc: 0.6968\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 453s 10ms/step - loss: 0.8355 - acc: 0.6923 - val_loss: 0.7954 - val_acc: 0.7038\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 442s 10ms/step - loss: 0.8052 - acc: 0.6997 - val_loss: 0.7769 - val_acc: 0.7107\n",
    "\n",
    "CPU times: user 1h 44min 42s, sys: 27min 2s, total: 2h 11min 45s\n",
    "Wall time: 29min 56s\n",
    "```\n",
    "[[ 594    2  138   26  282]\n",
    " [ 187    5  183   58  165]\n",
    " [  87    4  265  182  316]\n",
    " [  32    1  152  315 1107]\n",
    " [  58    0  103  206 6901]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.9327 - acc: 0.6701 - val_loss: 0.8315 - val_acc: 0.6937\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.8090 - acc: 0.7014 - val_loss: 0.7651 - val_acc: 0.7136\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7749 - acc: 0.7120 - val_loss: 0.7545 - val_acc: 0.7154\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7517 - acc: 0.7191 - val_loss: 0.7304 - val_acc: 0.7266\n",
    "\n",
    "CPU times: user 1h 13min 8s, sys: 3min 12s, total: 1h 16min 21s\n",
    "Wall time: 52min 17s\n",
    "```\n",
    "[[ 2006    61    99    24   411]\n",
    " [  683   137   236    63   373]\n",
    " [  491   129   450   293   768]\n",
    " [  191    43   255   731  2820]\n",
    " [  350    23   141   318 17327]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 1547s 7ms/step - loss: 0.8767 - acc: 0.6832 - val_loss: 0.7632 - val_acc: 0.7135\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 1550s 7ms/step - loss: 0.7758 - acc: 0.7119 - val_loss: 0.7332 - val_acc: 0.7257\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 1554s 7ms/step - loss: 0.7447 - acc: 0.7217 - val_loss: 0.7117 - val_acc: 0.7352\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 1555s 7ms/step - loss: 0.7239 - acc: 0.7285 - val_loss: 0.7185 - val_acc: 0.7317\n",
    "\n",
    "CPU times: user 2h 30min, sys: 7min 19s, total: 2h 37min 20s\n",
    "Wall time: 1h 43min 26s\n",
    "```\n",
    "[[ 3375   348   204    40  1231]\n",
    " [  917   517   424   120  1007]\n",
    " [  426   332   933   578  1988]\n",
    " [  152    47   351  1209  6286]\n",
    " [  257    46   131   364 35563]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
    "\n",
    "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
    "Wall time: 3h 26min 27s\n",
    "```\n",
    "[[ 8280   309   496   103  1266]\n",
    " [ 2909   560  1150   323  1012]\n",
    " [ 1727   466  2599  1514  2222]\n",
    " [  695   101  1245  3969 10121]\n",
    " [ 1443    63   622  2097 68399]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 13,409,297\n",
      "Trainable params: 13,409,169\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "rnn2 = Sequential()\n",
    "rnn2.add(embedding_layer2)\n",
    "rnn2.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(BatchNormalization())\n",
    "rnn2.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2, unroll=True))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/2\n",
      "240800/454761 [==============>...............] - ETA: 7:47 - loss: 0.7698 - acc: 0.7147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454761/454761 [==============================] - 1043s 2ms/step - loss: 0.7197 - acc: 0.7312 - val_loss: 0.6277 - val_acc: 0.7617\n",
      "Epoch 2/2\n",
      "454761/454761 [==============================] - 1042s 2ms/step - loss: 0.5856 - acc: 0.7798 - val_loss: 0.5968 - val_acc: 0.7757\n",
      "CPU times: user 35min 34s, sys: 1min 39s, total: 37min 14s\n",
      "Wall time: 34min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history2 = rnn2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn2.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7739  1300   605   123   687]\n",
      " [ 1551  2106  1584   291   422]\n",
      " [  558   949  4319  1738   964]\n",
      " [  224   165  1408  8047  6287]\n",
      " [  507   163   747  5231 65976]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = y_test-y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.9874 - acc: 0.6574 - val_loss: 0.9331 - val_acc: 0.6636\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.8625 - acc: 0.6846 - val_loss: 0.8611 - val_acc: 0.6892\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.8107 - acc: 0.6965 - val_loss: 0.8028 - val_acc: 0.6973\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.7780 - acc: 0.7080 - val_loss: 0.8506 - val_acc: 0.6994\n",
    "\n",
    "CPU times: user 56min 14s, sys: 14min 44s, total: 1h 10min 59s\n",
    "Wall time: 15min 49s\n",
    "```\n",
    "[[ 616   11   42   10  363]\n",
    " [ 198   10   64   39  287]\n",
    " [ 104    2   99   77  572]\n",
    " [  42    1   33   68 1463]\n",
    " [  58    0   25   27 7158]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 274s 2ms/step - loss: 0.9015 - acc: 0.6761 - val_loss: 0.8047 - val_acc: 0.6987\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7932 - acc: 0.7021 - val_loss: 0.7815 - val_acc: 0.7025\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7555 - acc: 0.7145 - val_loss: 0.8046 - val_acc: 0.6960\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7293 - acc: 0.7245 - val_loss: 0.7831 - val_acc: 0.7154\n",
    "\n",
    "CPU times: user 26min 3s, sys: 1min 13s, total: 27min 16s\n",
    "Wall time: 18min 13s\n",
    "```\n",
    "[[ 1551    62    85    23   880]\n",
    " [  509   110   230    70   573]\n",
    " [  237    91   373   326  1104]\n",
    " [   78    28   177   599  3158]\n",
    " [  128    13    83   233 17702]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 543s 2ms/step - loss: 0.8495 - acc: 0.6899 - val_loss: 0.7786 - val_acc: 0.7036\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7544 - acc: 0.7170 - val_loss: 0.7779 - val_acc: 0.6993\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7212 - acc: 0.7283 - val_loss: 0.7270 - val_acc: 0.7259\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 544s 2ms/step - loss: 0.6999 - acc: 0.7352 - val_loss: 0.7443 - val_acc: 0.7299\n",
    "\n",
    "CPU times: user 50min 21s, sys: 2min 18s, total: 52min 40s\n",
    "Wall time: 36min 18s\n",
    "```\n",
    "[[ 3708    70   204    67  1149]\n",
    " [ 1245   127   454   194   965]\n",
    " [  618   104   753   865  1917]\n",
    " [  173    18   270  1570  6014]\n",
    " [  313     4   102   606 35336]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1085s 2ms/step - loss: 0.8098 - acc: 0.7001 - val_loss: 0.7436 - val_acc: 0.7193\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.7243 - acc: 0.7271 - val_loss: 0.7504 - val_acc: 0.7266\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6966 - acc: 0.7364 - val_loss: 0.7103 - val_acc: 0.7338\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6782 - acc: 0.7429 - val_loss: 0.6988 - val_acc: 0.7354\n",
    "\n",
    "CPU times: user 1h 44min 8s, sys: 4min 53s, total: 1h 49min 1s\n",
    "Wall time: 1h 12min 31s\n",
    "```\n",
    "[[ 6899   726  1663    93  1073]\n",
    " [ 1535   781  2684   249   705]\n",
    " [  728   329  4397  1602  1472]\n",
    " [  266    47  2311  4983  8524]\n",
    " [  792    44  1657  3577 66554]]\n",
    " ```\n",
    " ## Training Embedding\n",
    " #### 2 Epochs\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/2\n",
    "454761/454761 [==============================] - 1038s 2ms/step - loss: 0.7174 - acc: 0.7310 - val_loss: 0.6536 - val_acc: 0.7576\n",
    "\n",
    "Epoch 2/2\n",
    "454761/454761 [==============================] - 1035s 2ms/step - loss: 0.5844 - acc: 0.7799 - val_loss: 0.5877 - val_acc: 0.7829\n",
    "\n",
    "CPU times: user 35min 26s, sys: 1min 36s, total: 37min 2s\n",
    "Wall time: 34min 43s\n",
    "```\n",
    "[[ 8332   882   598    50   592]\n",
    " [ 2046  1847  1444   209   408]\n",
    " [  791   827  4332  1484  1094]\n",
    " [  286   140  1557  6656  7492]\n",
    " [  762   130   839  3054 67839]]\n",
    " ```\n",
    " #### 4 Epochs\n",
    "Train on 454761 samples, validate on 113691 samples\n",
    "\n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1039s 2ms/step - loss: 0.7166 - acc: 0.7314 - val_loss: 0.6305 - val_acc: 0.7638\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.5836 - acc: 0.7805 - val_loss: 0.5858 - val_acc: 0.7810\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.5079 - acc: 0.8119 - val_loss: 0.5684 - val_acc: 0.7955\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.4453 - acc: 0.8384 - val_loss: 0.5913 - val_acc: 0.7899\n",
    "\n",
    "CPU times: user 1h 10min 36s, sys: 3min 19s, total: 1h 13min 56s\n",
    "Wall time: 1h 9min 17s\n",
    "```\n",
    "[[ 7640  1949   308    59   498]\n",
    " [ 1124  3690   665   112   363]\n",
    " [  604  1982  4115   965   862]\n",
    " [  274   498  1659  7581  6119]\n",
    " [  734   537  1011  3568 66774]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 13,409,037\n",
      "Trainable params: 13,408,909\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "rnn3 = Sequential()\n",
    "rnn3.add(embedding_layer3)\n",
    "rnn3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn3.add(MaxPooling1D(pool_size=2))\n",
    "rnn3.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn3.add(MaxPooling1D(pool_size=2))\n",
    "rnn3.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn3.add(MaxPooling1D(pool_size=2))\n",
    "rnn3.add(BatchNormalization())\n",
    "rnn3.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2, unroll=True))\n",
    "rnn3.add(Dense(64))\n",
    "rnn3.add(Dense(64))\n",
    "rnn3.add(Dense(1, activation='sigmoid'))\n",
    "rnn3.compile(loss='mean_squared_error',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ordinal = np.argmax(y_train_ohe, axis=1)/5\n",
    "y_test_ordinal = y_test/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/2\n",
      "454761/454761 [==============================] - 1053s 2ms/step - loss: 0.0252 - acc: 0.0808 - val_loss: 0.0247 - val_acc: 0.0890\n",
      "Epoch 2/2\n",
      "454761/454761 [==============================] - 1048s 2ms/step - loss: 0.0165 - acc: 0.0869 - val_loss: 0.0167 - val_acc: 0.0871\n"
     ]
    }
   ],
   "source": [
    "history3 = rnn3.fit(X_train, y_train_ordinal, validation_data=(X_test, y_test_ordinal), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rnn3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636636145341\n",
      "0.640719\n"
     ]
    }
   ],
   "source": [
    "print(y_test_ordinal.mean())\n",
    "print(y_hat.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-35310a50a1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_ordinal\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "err = y_test_ordinal-y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
