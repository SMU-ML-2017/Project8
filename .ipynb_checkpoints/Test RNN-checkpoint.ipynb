{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify on Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fd27bfb071c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Amazon Reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## https://www.kaggle.com/snap/amazon-fine-food-reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Reviews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n"
     ]
    }
   ],
   "source": [
    "## Amazon Reviews\n",
    "## https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "reviews = pd.read_csv('data/Reviews.csv')\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del reviews['ProfileName']\n",
    "except KeyError:\n",
    "    print('No such column')\n",
    "    \n",
    "try:\n",
    "    del reviews['Summary']\n",
    "except KeyError:\n",
    "    print('No such column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44736</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64421</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "44736  44737  B001EQ55RW  A2V0I904FH7ABY                     3   \n",
       "64421  64422  B000MIDROQ  A161DK06JJMCYF                     3   \n",
       "\n",
       "       HelpfulnessDenominator  Score        Time  \\\n",
       "44736                       2      4  1212883200   \n",
       "64421                       1      5  1224892800   \n",
       "\n",
       "                                                    Text  \n",
       "44736  It was almost a 'love at first bite' - the per...  \n",
       "64421  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove rows where numerator > denominator. Assume this is entry error in dataset.\n",
    "faulty_rows = reviews[reviews['HelpfulnessNumerator'] > reviews['HelpfulnessDenominator']]\n",
    "reviews = reviews[reviews['HelpfulnessNumerator'] <= reviews['HelpfulnessDenominator']]\n",
    "\n",
    "faulty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# [CITE] https://www.evanmiller.org/how-not-to-sort-by-average-rating.html\n",
    "# \"The lower bound of Wilson score confidence interval for a Bernoulli parameter\"\n",
    "def lbc(positive_votes, total_votes):\n",
    "    negative_votes = total_votes - positive_votes\n",
    "    if total_votes == 0:\n",
    "        return 0.0\n",
    "    lower_bound = ((positive_votes + 1.9208) / (total_votes) - 1.96 * math.sqrt((total_votes * negative_votes) / (total_votes) + 0.9604) / \n",
    "        (total_votes)) / (1 + 3.8416 / (total_votes))\n",
    "    return lower_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 80.2 ms, total: 14.7 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add the Lbc column\n",
    "lbc_for_row = lambda row: lbc(row['HelpfulnessNumerator'], row['HelpfulnessDenominator'])\n",
    "reviews['Lbc'] = reviews.apply(lbc_for_row, axis=1)\n",
    "\n",
    "# Make Score zero indexed\n",
    "reviews['Score'] = reviews['Score']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139166</th>\n",
       "      <td>139167</td>\n",
       "      <td>B0057OR5IO</td>\n",
       "      <td>A2K722XACWXW5G</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>All four of these had \"sell by\" dates which ha...</td>\n",
       "      <td>-0.170084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387988</th>\n",
       "      <td>387989</td>\n",
       "      <td>B007RTR8UM</td>\n",
       "      <td>A28I19Q54MYXGV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1340841600</td>\n",
       "      <td>The is a very low cost conditioner that goes o...</td>\n",
       "      <td>-0.170084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555251</th>\n",
       "      <td>555252</td>\n",
       "      <td>B002ESSASK</td>\n",
       "      <td>A1RJDQPF8G9WEP</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1307404800</td>\n",
       "      <td>We've been using this coffee for the past 2 mo...</td>\n",
       "      <td>-0.330250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443392</th>\n",
       "      <td>443393</td>\n",
       "      <td>B000MXJR7C</td>\n",
       "      <td>AP66BP6OX6WD1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1339718400</td>\n",
       "      <td>Please try to avoid , we were disappointed&lt;br ...</td>\n",
       "      <td>-0.170084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>8141</td>\n",
       "      <td>B0019GVYR2</td>\n",
       "      <td>ALI6SW10L0ZMC</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1324252800</td>\n",
       "      <td>Real salt is good and good for you. Doesnt tak...</td>\n",
       "      <td>-0.311735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "139166  139167  B0057OR5IO  A2K722XACWXW5G                     0   \n",
       "387988  387989  B007RTR8UM  A28I19Q54MYXGV                     0   \n",
       "555251  555252  B002ESSASK  A1RJDQPF8G9WEP                     0   \n",
       "443392  443393  B000MXJR7C   AP66BP6OX6WD1                     0   \n",
       "8140      8141  B0019GVYR2   ALI6SW10L0ZMC                     0   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "139166                       1      1  1337817600   \n",
       "387988                       1      2  1340841600   \n",
       "555251                       6      2  1307404800   \n",
       "443392                       1      0  1339718400   \n",
       "8140                         4      4  1324252800   \n",
       "\n",
       "                                                     Text       Lbc  \n",
       "139166  All four of these had \"sell by\" dates which ha... -0.170084  \n",
       "387988  The is a very low cost conditioner that goes o... -0.170084  \n",
       "555251  We've been using this coffee for the past 2 mo... -0.330250  \n",
       "443392  Please try to avoid , we were disappointed<br ... -0.170084  \n",
       "8140    Real salt is good and good for you. Doesnt tak... -0.311735  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reviews.sample(frac=.25)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of quartile 714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I highly recommend ALL Wellness products!! My Shih Tzu's were plaqued with severe allergy problems,, Spent $$$$ after $$$$ at vet for relief for them, nothing seemed to work.. I did extensive research on dog food and found out the mainstream brands are all crap, nothing but by products, additives, nothing natural about any of them.. Did research on Wellness and Blue Buffalo, look them up, you will find out for yourself.. Their products are all natural, no additives, no preservatives, 100% meat from animals that are NOT fed growth hormones, etc.. My dogs have been on this food, both wet and dry and they are thriving and the picture of health, bright eyes, shiny coats, full of energy and NO MORE allergies..\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartile = .85\n",
    "review_length = int(data['Text'].str.len().quantile(q=quartile))\n",
    "\n",
    "longest = data[data['Text'].str.len() == review_length]\n",
    "print(\"Length of quartile\", review_length)\n",
    "longest['Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72469 unique tokens. Distilled to 72469 top words.\n",
      "Shape of data tensor: (142113, 714)\n",
      "Shape of label tensor: (142113, 5)\n",
      "72469\n",
      "CPU times: user 20.6 s, sys: 522 ms, total: 21.1 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = 120000\n",
    "MAX_ART_LEN = int(data['Text'].str.len().quantile(q=quartile)) # maximum and minimum number of words \n",
    "                                                               #  based on a quartile of review length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(data.Text)\n",
    "sequences = tokenizer.texts_to_sequences(data.Text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(data['Score'])\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113690, 714) (113690, 5)\n",
      "[ 10426.   5908.   8574.  16131.  72651.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=data['Score'], \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = y_ohe.shape[1]\n",
    "print(X_train.shape,y_train_ohe.shape)\n",
    "print(np.sum(y_train_ohe,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(72470, 100)\n",
      "CPU times: user 12.3 s, sys: 375 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# Define the embeding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 714, 100)          7247000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 714, 32)           6432      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               39900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 7,293,837\n",
      "Trainable params: 46,837\n",
      "Non-trainable params: 7,247,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import GRU                  # Recurrent Layers\n",
    "\n",
    "rnn1 = Sequential()\n",
    "rnn1.add(embedding_layer)\n",
    "rnn1.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "rnn1.add(MaxPooling1D(pool_size=2))\n",
    "rnn1.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn1.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113690 samples, validate on 28423 samples\n",
      "Epoch 1/4\n",
      "113690/113690 [==============================] - 1109s 10ms/step - loss: 0.9354 - acc: 0.6694 - val_loss: 0.8149 - val_acc: 0.6986\n",
      "Epoch 2/4\n",
      "113690/113690 [==============================] - 1797s 16ms/step - loss: 0.8237 - acc: 0.6959 - val_loss: 0.7923 - val_acc: 0.7062\n",
      "Epoch 3/4\n",
      "113690/113690 [==============================] - 1177s 10ms/step - loss: 0.7890 - acc: 0.7080 - val_loss: 0.7639 - val_acc: 0.7157\n",
      "Epoch 4/4\n",
      "113690/113690 [==============================] - 1176s 10ms/step - loss: 0.7669 - acc: 0.7156 - val_loss: 0.7474 - val_acc: 0.7211\n",
      "CPU times: user 4h 28min 29s, sys: 1h 8min 6s, total: 5h 36min 36s\n",
      "Wall time: 1h 27min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c944908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rnn1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn1.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2007    31   124    19   425]\n",
      " [  715    52   306    60   344]\n",
      " [  448    41   550   317   788]\n",
      " [  212    13   342   665  2801]\n",
      " [  488     4   162   288 17221]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 451s 10ms/step - loss: 1.0291 - acc: 0.6508 - val_loss: 0.8893 - val_acc: 0.6783\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 448s 10ms/step - loss: 0.8846 - acc: 0.6804 - val_loss: 0.8173 - val_acc: 0.6968\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 453s 10ms/step - loss: 0.8355 - acc: 0.6923 - val_loss: 0.7954 - val_acc: 0.7038\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 442s 10ms/step - loss: 0.8052 - acc: 0.6997 - val_loss: 0.7769 - val_acc: 0.7107\n",
    "\n",
    "CPU times: user 1h 44min 42s, sys: 27min 2s, total: 2h 11min 45s\n",
    "Wall time: 29min 56s\n",
    "```\n",
    "[[ 594    2  138   26  282]\n",
    " [ 187    5  183   58  165]\n",
    " [  87    4  265  182  316]\n",
    " [  32    1  152  315 1107]\n",
    " [  58    0  103  206 6901]]\n",
    "```\n",
    "#### 25% of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 714, 100)          7247000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 714, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 7,352,397\n",
      "Trainable params: 105,269\n",
      "Non-trainable params: 7,247,128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "rnn2 = Sequential()\n",
    "rnn2.add(embedding_layer)\n",
    "rnn2.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(BatchNormalization())\n",
    "rnn2.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113690 samples, validate on 28423 samples\n",
      "Epoch 1/4\n",
      "113690/113690 [==============================] - 715s 6ms/step - loss: 0.9120 - acc: 0.6730 - val_loss: 0.8740 - val_acc: 0.6931\n",
      "Epoch 2/4\n",
      " 87232/113690 [======================>.......] - ETA: 7:59 - loss: 0.8064 - acc: 0.7008"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rnn2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn2.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.9874 - acc: 0.6574 - val_loss: 0.9331 - val_acc: 0.6636\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.8625 - acc: 0.6846 - val_loss: 0.8611 - val_acc: 0.6892\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.8107 - acc: 0.6965 - val_loss: 0.8028 - val_acc: 0.6973\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.7780 - acc: 0.7080 - val_loss: 0.8506 - val_acc: 0.6994\n",
    "\n",
    "CPU times: user 56min 14s, sys: 14min 44s, total: 1h 10min 59s\n",
    "Wall time: 15min 49s\n",
    "```\n",
    "[[ 616   11   42   10  363]\n",
    " [ 198   10   64   39  287]\n",
    " [ 104    2   99   77  572]\n",
    " [  42    1   33   68 1463]\n",
    " [  58    0   25   27 7158]]\n",
    "```\n",
    "#### 25% of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
