{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify on Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      "Id                        568454 non-null int64\n",
      "ProductId                 568454 non-null object\n",
      "UserId                    568454 non-null object\n",
      "ProfileName               568438 non-null object\n",
      "HelpfulnessNumerator      568454 non-null int64\n",
      "HelpfulnessDenominator    568454 non-null int64\n",
      "Score                     568454 non-null int64\n",
      "Time                      568454 non-null int64\n",
      "Summary                   568427 non-null object\n",
      "Text                      568454 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "## Amazon Reviews\n",
    "## https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "reviews = pd.read_csv('data/Reviews.csv')\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del reviews['ProfileName']\n",
    "except KeyError:\n",
    "    print('No such column')\n",
    "    \n",
    "try:\n",
    "    del reviews['Summary']\n",
    "except KeyError:\n",
    "    print('No such column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44736</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64421</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "44736  44737  B001EQ55RW  A2V0I904FH7ABY                     3   \n",
       "64421  64422  B000MIDROQ  A161DK06JJMCYF                     3   \n",
       "\n",
       "       HelpfulnessDenominator  Score        Time  \\\n",
       "44736                       2      4  1212883200   \n",
       "64421                       1      5  1224892800   \n",
       "\n",
       "                                                    Text  \n",
       "44736  It was almost a 'love at first bite' - the per...  \n",
       "64421  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove rows where numerator > denominator. Assume this is entry error in dataset.\n",
    "faulty_rows = reviews[reviews['HelpfulnessNumerator'] > reviews['HelpfulnessDenominator']]\n",
    "reviews = reviews[reviews['HelpfulnessNumerator'] <= reviews['HelpfulnessDenominator']]\n",
    "\n",
    "faulty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# [CITE] https://www.evanmiller.org/how-not-to-sort-by-average-rating.html\n",
    "# \"The lower bound of Wilson score confidence interval for a Bernoulli parameter\"\n",
    "def lbc(positive_votes, total_votes):\n",
    "    negative_votes = total_votes - positive_votes\n",
    "    if total_votes == 0:\n",
    "        return 0.0\n",
    "    lower_bound = ((positive_votes + 1.9208) / (total_votes) - 1.96 * math.sqrt((total_votes * negative_votes) / (total_votes) + 0.9604) / \n",
    "        (total_votes)) / (1 + 3.8416 / (total_votes))\n",
    "    return lower_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.31 s, sys: 18.2 ms, total: 8.33 s\n",
      "Wall time: 8.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add the Lbc column\n",
    "lbc_for_row = lambda row: lbc(row['HelpfulnessNumerator'], row['HelpfulnessDenominator'])\n",
    "reviews['Lbc'] = reviews.apply(lbc_for_row, axis=1)\n",
    "\n",
    "# Make Score zero indexed\n",
    "reviews['Score'] = reviews['Score']-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>354161</th>\n",
       "      <td>354162</td>\n",
       "      <td>B002TM5458</td>\n",
       "      <td>A2833MXJLALLQ2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1328486400</td>\n",
       "      <td>I was a bit hesitant when I first ordered this...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>5107</td>\n",
       "      <td>B001HOUGFC</td>\n",
       "      <td>A2SURZ7AWPTR5P</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1323475200</td>\n",
       "      <td>Product arrived on time, nicely packaged and g...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392096</th>\n",
       "      <td>392097</td>\n",
       "      <td>B0013ES5WC</td>\n",
       "      <td>A50MZWLPWJ2N3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1307491200</td>\n",
       "      <td>This is my favorite fountain. My cats love it ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133823</th>\n",
       "      <td>133824</td>\n",
       "      <td>B002AR14WO</td>\n",
       "      <td>A3R33KMESG0XGQ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1255564800</td>\n",
       "      <td>Being an avid tea drinker, I simply couldn't p...</td>\n",
       "      <td>0.206543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120402</th>\n",
       "      <td>120403</td>\n",
       "      <td>B005K4Q37A</td>\n",
       "      <td>ADSGJIX2XGP5K</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1332547200</td>\n",
       "      <td>Threw the entire box away.  I thought I was bu...</td>\n",
       "      <td>0.558849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "354161  354162  B002TM5458  A2833MXJLALLQ2                     0   \n",
       "5106      5107  B001HOUGFC  A2SURZ7AWPTR5P                     0   \n",
       "392096  392097  B0013ES5WC   A50MZWLPWJ2N3                     0   \n",
       "133823  133824  B002AR14WO  A3R33KMESG0XGQ                     1   \n",
       "120402  120403  B005K4Q37A   ADSGJIX2XGP5K                     8   \n",
       "\n",
       "        HelpfulnessDenominator  Score        Time  \\\n",
       "354161                       0      4  1328486400   \n",
       "5106                         0      4  1323475200   \n",
       "392096                       0      4  1307491200   \n",
       "133823                       1      4  1255564800   \n",
       "120402                       9      0  1332547200   \n",
       "\n",
       "                                                     Text       Lbc  \n",
       "354161  I was a bit hesitant when I first ordered this...  0.000000  \n",
       "5106    Product arrived on time, nicely packaged and g...  0.000000  \n",
       "392096  This is my favorite fountain. My cats love it ...  0.000000  \n",
       "133823  Being an avid tea drinker, I simply couldn't p...  0.206543  \n",
       "120402  Threw the entire box away.  I thought I was bu...  0.558849  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = reviews.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of quartile 715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is really a great tasting cereal.  The Apple and Acai combine for a strong fruity taste to go along with the crunchy, granular texture of the granola that you\\'re probably familiar with. It\\'s a big sweeter than I would prefer for daily consumption, so I don\\'t eat it everyday.<br /><br />Being granola, this is a fairly dense food so while 11.5oz might seem like a decent amount of cereal, the box is actually pretty small.  I get about three bowls of cereal out of a box, though granted I like a larger breakfast.  Compared to other cereals, it\\'s a tax expensive, but I don\\'t mind (it\\'s $3.30/box at my local Kroger).  I use it as an occasional \"treat\" in the morning for when I don\\'t feel like eating oatmeal.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartile = .85\n",
    "review_length = int(data['Text'].str.len().quantile(q=quartile))\n",
    "\n",
    "longest = data[data['Text'].str.len() == review_length]\n",
    "print(\"Length of quartile\", review_length)\n",
    "longest['Text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133038 unique tokens. Distilled to 120000 top words.\n",
      "Shape of data tensor: (568452, 715)\n",
      "Shape of label tensor: (568452, 5)\n",
      "119999\n",
      "CPU times: user 43.2 s, sys: 764 ms, total: 44 s\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = 120000\n",
    "MAX_ART_LEN = int(data['Text'].str.len().quantile(q=quartile)) # maximum and minimum number of words \n",
    "                                                               #  based on a quartile of review length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(data.Text)\n",
    "sequences = tokenizer.texts_to_sequences(data.Text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(data['Score'])\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454761, 715) (454761, 5)\n",
      "[  41814.   23815.   34112.   64523.  290497.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=data['Score'], \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = y_ohe.shape[1]\n",
    "print(X_train.shape,y_train_ohe.shape)\n",
    "print(np.sum(y_train_ohe,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(133039, 100)\n",
      "CPU times: user 7.07 s, sys: 118 ms, total: 7.19 s\n",
      "Wall time: 7.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# Define the embeding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer1 = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "embedding_layer2 = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "embedding_layer3 = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               39900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 13,353,937\n",
      "Trainable params: 50,037\n",
      "Non-trainable params: 13,303,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import GRU                  # Recurrent Layers\n",
    "\n",
    "rnn1 = Sequential()\n",
    "rnn1.add(embedding_layer1)\n",
    "rnn1.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn1.add(MaxPooling1D(pool_size=2))\n",
    "rnn1.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn1.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/4\n",
      "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
      "Epoch 2/4\n",
      "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
      "Epoch 3/4\n",
      "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
      "Epoch 4/4\n",
      "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
      "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
      "Wall time: 3h 26min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44ef714e48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history1 = rnn1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn1.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8280   309   496   103  1266]\n",
      " [ 2909   560  1150   323  1012]\n",
      " [ 1727   466  2599  1514  2222]\n",
      " [  695   101  1245  3969 10121]\n",
      " [ 1443    63   622  2097 68399]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 451s 10ms/step - loss: 1.0291 - acc: 0.6508 - val_loss: 0.8893 - val_acc: 0.6783\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 448s 10ms/step - loss: 0.8846 - acc: 0.6804 - val_loss: 0.8173 - val_acc: 0.6968\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 453s 10ms/step - loss: 0.8355 - acc: 0.6923 - val_loss: 0.7954 - val_acc: 0.7038\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 442s 10ms/step - loss: 0.8052 - acc: 0.6997 - val_loss: 0.7769 - val_acc: 0.7107\n",
    "\n",
    "CPU times: user 1h 44min 42s, sys: 27min 2s, total: 2h 11min 45s\n",
    "Wall time: 29min 56s\n",
    "```\n",
    "[[ 594    2  138   26  282]\n",
    " [ 187    5  183   58  165]\n",
    " [  87    4  265  182  316]\n",
    " [  32    1  152  315 1107]\n",
    " [  58    0  103  206 6901]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.9327 - acc: 0.6701 - val_loss: 0.8315 - val_acc: 0.6937\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 785s 7ms/step - loss: 0.8090 - acc: 0.7014 - val_loss: 0.7651 - val_acc: 0.7136\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7749 - acc: 0.7120 - val_loss: 0.7545 - val_acc: 0.7154\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 784s 7ms/step - loss: 0.7517 - acc: 0.7191 - val_loss: 0.7304 - val_acc: 0.7266\n",
    "\n",
    "CPU times: user 1h 13min 8s, sys: 3min 12s, total: 1h 16min 21s\n",
    "Wall time: 52min 17s\n",
    "```\n",
    "[[ 2006    61    99    24   411]\n",
    " [  683   137   236    63   373]\n",
    " [  491   129   450   293   768]\n",
    " [  191    43   255   731  2820]\n",
    " [  350    23   141   318 17327]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 1547s 7ms/step - loss: 0.8767 - acc: 0.6832 - val_loss: 0.7632 - val_acc: 0.7135\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 1550s 7ms/step - loss: 0.7758 - acc: 0.7119 - val_loss: 0.7332 - val_acc: 0.7257\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 1554s 7ms/step - loss: 0.7447 - acc: 0.7217 - val_loss: 0.7117 - val_acc: 0.7352\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 1555s 7ms/step - loss: 0.7239 - acc: 0.7285 - val_loss: 0.7185 - val_acc: 0.7317\n",
    "\n",
    "CPU times: user 2h 30min, sys: 7min 19s, total: 2h 37min 20s\n",
    "Wall time: 1h 43min 26s\n",
    "```\n",
    "[[ 3375   348   204    40  1231]\n",
    " [  917   517   424   120  1007]\n",
    " [  426   332   933   578  1988]\n",
    " [  152    47   351  1209  6286]\n",
    " [  257    46   131   364 35563]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 3094s 7ms/step - loss: 0.8326 - acc: 0.6952 - val_loss: 0.7404 - val_acc: 0.7233\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 3096s 7ms/step - loss: 0.7582 - acc: 0.7169 - val_loss: 0.7318 - val_acc: 0.7235\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 3098s 7ms/step - loss: 0.7566 - acc: 0.7174 - val_loss: 0.7054 - val_acc: 0.7344\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 3099s 7ms/step - loss: 0.7325 - acc: 0.7254 - val_loss: 0.6979 - val_acc: 0.7371\n",
    "\n",
    "CPU times: user 4h 57min 53s, sys: 13min 48s, total: 5h 11min 42s\n",
    "Wall time: 3h 26min 27s\n",
    "```\n",
    "[[ 8280   309   496   103  1266]\n",
    " [ 2909   560  1150   323  1012]\n",
    " [ 1727   466  2599  1514  2222]\n",
    " [  695   101  1245  3969 10121]\n",
    " [ 1443    63   622  2097 68399]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 13,409,297\n",
      "Trainable params: 13,409,169\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "rnn2 = Sequential()\n",
    "rnn2.add(embedding_layer2)\n",
    "rnn2.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn2.add(MaxPooling1D(pool_size=2))\n",
    "rnn2.add(BatchNormalization())\n",
    "rnn2.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2, unroll=True))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(64))\n",
    "rnn2.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/2\n",
      "240800/454761 [==============>...............] - ETA: 7:47 - loss: 0.7698 - acc: 0.7147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454761/454761 [==============================] - 1043s 2ms/step - loss: 0.7197 - acc: 0.7312 - val_loss: 0.6277 - val_acc: 0.7617\n",
      "Epoch 2/2\n",
      "454761/454761 [==============================] - 1042s 2ms/step - loss: 0.5856 - acc: 0.7798 - val_loss: 0.5968 - val_acc: 0.7757\n",
      "CPU times: user 35min 34s, sys: 1min 39s, total: 37min 14s\n",
      "Wall time: 34min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history2 = rnn2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.argmax(rnn2.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7739  1300   605   123   687]\n",
      " [ 1551  2106  1584   291   422]\n",
      " [  558   949  4319  1738   964]\n",
      " [  224   165  1408  8047  6287]\n",
      " [  507   163   747  5231 65976]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = np.argmax(y_test_ohe, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = y_test-y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "#### 10% of Dataset\n",
    "Train on 45476 samples, validate on 11369 samples\n",
    "\n",
    "Epoch 1/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.9874 - acc: 0.6574 - val_loss: 0.9331 - val_acc: 0.6636\n",
    "\n",
    "Epoch 2/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.8625 - acc: 0.6846 - val_loss: 0.8611 - val_acc: 0.6892\n",
    "\n",
    "Epoch 3/4\n",
    "45476/45476 [==============================] - 237s 5ms/step - loss: 0.8107 - acc: 0.6965 - val_loss: 0.8028 - val_acc: 0.6973\n",
    "\n",
    "Epoch 4/4\n",
    "45476/45476 [==============================] - 236s 5ms/step - loss: 0.7780 - acc: 0.7080 - val_loss: 0.8506 - val_acc: 0.6994\n",
    "\n",
    "CPU times: user 56min 14s, sys: 14min 44s, total: 1h 10min 59s\n",
    "Wall time: 15min 49s\n",
    "```\n",
    "[[ 616   11   42   10  363]\n",
    " [ 198   10   64   39  287]\n",
    " [ 104    2   99   77  572]\n",
    " [  42    1   33   68 1463]\n",
    " [  58    0   25   27 7158]]\n",
    "```\n",
    "#### 25% of Dataset\n",
    "Train on 113690 samples, validate on 28423 samples\n",
    "\n",
    "Epoch 1/4\n",
    "113690/113690 [==============================] - 274s 2ms/step - loss: 0.9015 - acc: 0.6761 - val_loss: 0.8047 - val_acc: 0.6987\n",
    "\n",
    "Epoch 2/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7932 - acc: 0.7021 - val_loss: 0.7815 - val_acc: 0.7025\n",
    "\n",
    "Epoch 3/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7555 - acc: 0.7145 - val_loss: 0.8046 - val_acc: 0.6960\n",
    "\n",
    "Epoch 4/4\n",
    "113690/113690 [==============================] - 273s 2ms/step - loss: 0.7293 - acc: 0.7245 - val_loss: 0.7831 - val_acc: 0.7154\n",
    "\n",
    "CPU times: user 26min 3s, sys: 1min 13s, total: 27min 16s\n",
    "Wall time: 18min 13s\n",
    "```\n",
    "[[ 1551    62    85    23   880]\n",
    " [  509   110   230    70   573]\n",
    " [  237    91   373   326  1104]\n",
    " [   78    28   177   599  3158]\n",
    " [  128    13    83   233 17702]]\n",
    " ```\n",
    " #### 50% of Dataset\n",
    " Train on 227380 samples, validate on 56846 samples\n",
    " \n",
    "Epoch 1/4\n",
    "227380/227380 [==============================] - 543s 2ms/step - loss: 0.8495 - acc: 0.6899 - val_loss: 0.7786 - val_acc: 0.7036\n",
    "\n",
    "Epoch 2/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7544 - acc: 0.7170 - val_loss: 0.7779 - val_acc: 0.6993\n",
    "\n",
    "Epoch 3/4\n",
    "227380/227380 [==============================] - 545s 2ms/step - loss: 0.7212 - acc: 0.7283 - val_loss: 0.7270 - val_acc: 0.7259\n",
    "\n",
    "Epoch 4/4\n",
    "227380/227380 [==============================] - 544s 2ms/step - loss: 0.6999 - acc: 0.7352 - val_loss: 0.7443 - val_acc: 0.7299\n",
    "\n",
    "CPU times: user 50min 21s, sys: 2min 18s, total: 52min 40s\n",
    "Wall time: 36min 18s\n",
    "```\n",
    "[[ 3708    70   204    67  1149]\n",
    " [ 1245   127   454   194   965]\n",
    " [  618   104   753   865  1917]\n",
    " [  173    18   270  1570  6014]\n",
    " [  313     4   102   606 35336]]\n",
    " ```\n",
    " #### 100% of Dataset\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1085s 2ms/step - loss: 0.8098 - acc: 0.7001 - val_loss: 0.7436 - val_acc: 0.7193\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.7243 - acc: 0.7271 - val_loss: 0.7504 - val_acc: 0.7266\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6966 - acc: 0.7364 - val_loss: 0.7103 - val_acc: 0.7338\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1089s 2ms/step - loss: 0.6782 - acc: 0.7429 - val_loss: 0.6988 - val_acc: 0.7354\n",
    "\n",
    "CPU times: user 1h 44min 8s, sys: 4min 53s, total: 1h 49min 1s\n",
    "Wall time: 1h 12min 31s\n",
    "```\n",
    "[[ 6899   726  1663    93  1073]\n",
    " [ 1535   781  2684   249   705]\n",
    " [  728   329  4397  1602  1472]\n",
    " [  266    47  2311  4983  8524]\n",
    " [  792    44  1657  3577 66554]]\n",
    " ```\n",
    " ## Training Embedding\n",
    " #### 2 Epochs\n",
    " Train on 454761 samples, validate on 113691 samples\n",
    " \n",
    "Epoch 1/2\n",
    "454761/454761 [==============================] - 1038s 2ms/step - loss: 0.7174 - acc: 0.7310 - val_loss: 0.6536 - val_acc: 0.7576\n",
    "\n",
    "Epoch 2/2\n",
    "454761/454761 [==============================] - 1035s 2ms/step - loss: 0.5844 - acc: 0.7799 - val_loss: 0.5877 - val_acc: 0.7829\n",
    "\n",
    "CPU times: user 35min 26s, sys: 1min 36s, total: 37min 2s\n",
    "Wall time: 34min 43s\n",
    "```\n",
    "[[ 8332   882   598    50   592]\n",
    " [ 2046  1847  1444   209   408]\n",
    " [  791   827  4332  1484  1094]\n",
    " [  286   140  1557  6656  7492]\n",
    " [  762   130   839  3054 67839]]\n",
    " ```\n",
    " #### 4 Epochs\n",
    "Train on 454761 samples, validate on 113691 samples\n",
    "\n",
    "Epoch 1/4\n",
    "454761/454761 [==============================] - 1039s 2ms/step - loss: 0.7166 - acc: 0.7314 - val_loss: 0.6305 - val_acc: 0.7638\n",
    "\n",
    "Epoch 2/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.5836 - acc: 0.7805 - val_loss: 0.5858 - val_acc: 0.7810\n",
    "\n",
    "Epoch 3/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.5079 - acc: 0.8119 - val_loss: 0.5684 - val_acc: 0.7955\n",
    "\n",
    "Epoch 4/4\n",
    "454761/454761 [==============================] - 1036s 2ms/step - loss: 0.4453 - acc: 0.8384 - val_loss: 0.5913 - val_acc: 0.7899\n",
    "\n",
    "CPU times: user 1h 10min 36s, sys: 3min 19s, total: 1h 13min 56s\n",
    "Wall time: 1h 9min 17s\n",
    "```\n",
    "[[ 7640  1949   308    59   498]\n",
    " [ 1124  3690   665   112   363]\n",
    " [  604  1982  4115   965   862]\n",
    " [  274   498  1659  7581  6119]\n",
    " [  734   537  1011  3568 66774]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 715, 100)          13303900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 357, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 357, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 178, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 178, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 89, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 89, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 13,409,037\n",
      "Trainable params: 13,408,909\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D # Convolution Layers\n",
    "from keras.layers import Dense                # Dense Layers\n",
    "from keras.layers import LSTM                 # Recurrent Layers\n",
    "\n",
    "rnn3 = Sequential()\n",
    "rnn3.add(embedding_layer3)\n",
    "rnn3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn3.add(MaxPooling1D(pool_size=2))\n",
    "rnn3.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn3.add(MaxPooling1D(pool_size=2))\n",
    "rnn3.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "rnn3.add(MaxPooling1D(pool_size=2))\n",
    "rnn3.add(BatchNormalization())\n",
    "rnn3.add(LSTM(100,dropout=0.25, recurrent_dropout=0.2))\n",
    "rnn3.add(Dense(64))\n",
    "rnn3.add(Dense(64))\n",
    "rnn3.add(Dense(1, activation='sigmoid'))\n",
    "rnn3.compile(loss='mean_squared_error',\n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ordinal = np.argmax(y_train_ohe, axis=1)/5\n",
    "y_test_ordinal = np.argmax(y_test_ohe, axis=1)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from keras.models import load_model\n",
    "import json\n",
    "\n",
    "def cache_fit(model_name: str, model, *args, **kwargs):\n",
    "    archive_name = f'{model_name}_model.h5'\n",
    "    history_name = f'{model_name}_history.json'\n",
    "    archive_exists = os.path.isfile(archive_name)\n",
    "\n",
    "    if not archive_exists:\n",
    "        print(f'Model {model_name} not found in archive. Training new model.')\n",
    "        hist = model.fit(*args, **kwargs)\n",
    "        model.save(archive_name)\n",
    "        with open(history_name, 'w') as f:\n",
    "            json.dump(hist.history, f)\n",
    "        return model\n",
    "    else:\n",
    "        print('Model found on disk. Reloading.')\n",
    "        return load_model(archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found on disk. Reloading.\n"
     ]
    }
   ],
   "source": [
    "model = cache_fit(\n",
    "    'rnn3', rnn3, \n",
    "    X_train, y_train_ordinal, validation_data=(X_test, y_test_ordinal), epochs=2, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 454761 samples, validate on 113691 samples\n",
      "Epoch 1/2\n",
      "454761/454761 [==============================] - 1246s 3ms/step - loss: 0.0254 - acc: 0.0805 - val_loss: 0.0208 - val_acc: 0.0807\n",
      "Epoch 2/2\n",
      "454761/454761 [==============================] - 1246s 3ms/step - loss: 0.0166 - acc: 0.0869 - val_loss: 0.0165 - val_acc: 0.0865\n",
      "CPU times: user 55min 51s, sys: 3min 19s, total: 59min 10s\n",
      "Wall time: 41min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history3 = rnn3.fit(X_train, y_train_ordinal, validation_data=(X_test, y_test_ordinal), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rnn3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636636145341\n",
      "0.635671\n"
     ]
    }
   ],
   "source": [
    "print(y_test_ordinal.mean())\n",
    "print(y_hat.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-641d972ad03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_test_ordinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "err = sum(abs(y_hat-y_test_ordinal)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
